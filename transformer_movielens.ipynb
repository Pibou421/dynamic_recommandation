{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mean_of_user</th>\n",
       "      <th>scaled_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>0.633621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>0.633621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>609</td>\n",
       "      <td>9416</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>0.311444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>609</td>\n",
       "      <td>9443</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>609</td>\n",
       "      <td>9444</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>609</td>\n",
       "      <td>9445</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>609</td>\n",
       "      <td>9485</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>-0.688556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp  mean_of_user  scaled_rating\n",
       "0            0        0     4.0   964982703      4.366379      -0.366379\n",
       "1            0        2     4.0   964981247      4.366379      -0.366379\n",
       "2            0        5     4.0   964982224      4.366379      -0.366379\n",
       "3            0       43     5.0   964983815      4.366379       0.633621\n",
       "4            0       46     5.0   964982931      4.366379       0.633621\n",
       "...        ...      ...     ...         ...           ...            ...\n",
       "100831     609     9416     4.0  1493848402      3.688556       0.311444\n",
       "100832     609     9443     5.0  1493850091      3.688556       1.311444\n",
       "100833     609     9444     5.0  1494273047      3.688556       1.311444\n",
       "100834     609     9445     5.0  1493846352      3.688556       1.311444\n",
       "100835     609     9485     3.0  1493846415      3.688556      -0.688556\n",
       "\n",
       "[100836 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_table = pd.read_csv('ratings.csv')\n",
    "ratings_table[\"userId\"] = np.unique(ratings_table[\"userId\"], return_inverse=True)[1]\n",
    "ratings_table[\"movieId\"] = np.unique(ratings_table[\"movieId\"], return_inverse=True)[1]\n",
    "ratings_table[\"mean_of_user\"] = ratings_table.groupby('userId')[\"rating\"].transform('mean')\n",
    "ratings_table[\"scaled_rating\"] = (ratings_table[\"rating\"] - ratings_table[\"mean_of_user\"])\n",
    "ratings_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zero.sgd_temporal import MangakiSGDTemporal\n",
    "import random\n",
    "\n",
    "users = np.array(ratings_table[['userId']])[:,0]\n",
    "items =np.array(ratings_table[['movieId']])[:,0]\n",
    "ratings =np.array(ratings_table[['rating']])[:,0]\n",
    "timestamps =np.array(ratings_table[['timestamp']])[:,0]\n",
    "mean_of_user = np.array(ratings_table[['mean_of_user']])[:,0]\n",
    "scaled_ratings = np.array(ratings_table[['scaled_rating']])[:,0]\n",
    "\n",
    "def add_in_ordered_list_of_3_uplets(list, item):\n",
    "    (a,b,c) = item\n",
    "    i=0\n",
    "    while i<len(list) and c > list[i][2] :\n",
    "        i+=1\n",
    "    list.insert(i, item)\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    newl = []\n",
    "    for x in l :\n",
    "        if x not in newl:\n",
    "            newl.append(x)\n",
    "    return newl\n",
    "\n",
    "def list_into_dictionnaries(users, items, ratings, timestamps):\n",
    "    users_dict = {}\n",
    "    test_dict = {}\n",
    "    users_individual = remove_duplicates(users)\n",
    "    random.shuffle(users_individual)\n",
    "    nb_users = len(users_individual)\n",
    "    training_users = users_individual[:math.floor(0.8*nb_users)]\n",
    "    testing_users = users_individual[math.floor(0.8*nb_users):]\n",
    "    for (i,j,r, t) in zip(users, items, ratings, timestamps) :\n",
    "        if i in training_users:\n",
    "            if i in users_dict :\n",
    "                add_in_ordered_list_of_3_uplets(users_dict[i],(j,r,t))\n",
    "            else :\n",
    "                users_dict[i] = [(j,r,t)]\n",
    "        else :\n",
    "            if i in test_dict :\n",
    "                add_in_ordered_list_of_3_uplets(test_dict[i],(j,r,t))\n",
    "            else :\n",
    "                test_dict[i] = [(j,r,t)]\n",
    "    return (users_dict, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dict,test_dict = list_into_dictionnaries(users, items, ratings, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_list = []\n",
    "\n",
    "for i in users_dict.keys() :\n",
    "    for (movie, rating, t) in users_dict[i]:\n",
    "        if rating > 3.0:\n",
    "            sequences_list.append(str(movie) + \"+\")\n",
    "        else :\n",
    "            sequences_list.append(str(movie) + \"-\")\n",
    "    sequences_list.append(\".\")\n",
    "\n",
    "#sequences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_string = \"\"\n",
    "\n",
    "for x in sequences_list:\n",
    "    sequences_string = sequences_string + x + \" \"\n",
    "\n",
    "#sequences_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torchtext\\nTEXT = torchtext.data.Field(init_token='<sos>',\\n                            eos_token='<eos>',\\n                            lower=True)\\n\\n\\nTEXT.build_vocab(sequences_list)\\ntrain_txt = TEXT.numericalize(sequences_list)\\nval_txt = test_txt = train_txt\\nprint(TEXT.numericalize(sequences_string))\\nprint(len(sequences_list))\\nprint(train_txt.shape)\\nprint('long', len(TEXT.vocab.stoi))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torchtext\n",
    "TEXT = torchtext.data.Field(init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "\n",
    "\n",
    "TEXT.build_vocab(sequences_list)\n",
    "train_txt = TEXT.numericalize(sequences_list)\n",
    "val_txt = test_txt = train_txt\n",
    "print(TEXT.numericalize(sequences_string))\n",
    "print(len(sequences_list))\n",
    "print(train_txt.shape)\n",
    "print('long', len(TEXT.vocab.stoi))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_txt = torch.transpose(train_txt, 0, 1)\\ntrain_txt = torch.reshape(train_txt, (100,84133))\\nval_txt = train_txt\\ntest_txt = train_txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_txt = torch.transpose(train_txt, 0, 1)\n",
    "train_txt = torch.reshape(train_txt, (100,84133))\n",
    "val_txt = train_txt\n",
    "test_txt = train_txt'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'converter(train_txt)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''converter(train_txt)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(TEXT.numericalize(train))\\nprint('long', len(TEXT.vocab.stoi))\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(TEXT.numericalize(train))\n",
    "print('long', len(TEXT.vocab.stoi))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n",
      "827\n",
      "827\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = sequences_list, sequences_list, sequences_list\n",
    "TEXT.build_vocab([sequences_list])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    print(nbatch)\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 100\n",
    "eval_batch_size = 100\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "#train_data = torch.transpose(train_data, 0, 1)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([827, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_data = torch.reshape(train_data, (84133,100))\n",
    "#train_data.shape\n",
    "#val_data = train_data\n",
    "#test_data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = torch.transpose(train_data, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate input and target sequence\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dico_file = open(\"dico_fr.txt\", \\'r\\')\\ndico_string = dico_file.read()\\ndico_list = dico_string.split(\\'\\n\\')\\nlists_of_letters = []\\nfor x in dico_list:\\n    word_list = [ord(i)-ord(\\' \\') for i in x.rjust(25)]\\n    lists_of_letters.append(word_list)    \\ntrain_data = torch.tensor(lists_of_letters[:100])\\nval_data = train_data\\ntest_data = train_data\\nprint(train_data)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dico_file = open(\"dico_fr.txt\", 'r')\n",
    "dico_string = dico_file.read()\n",
    "dico_list = dico_string.split('\\n')\n",
    "lists_of_letters = []\n",
    "for x in dico_list:\n",
    "    word_list = [ord(i)-ord(' ') for i in x.rjust(25)]\n",
    "    lists_of_letters.append(word_list)    \n",
    "train_data = torch.tensor(lists_of_letters[:100])\n",
    "val_data = train_data\n",
    "test_data = train_data\n",
    "print(train_data)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "![](../_static/img/transformer_input_target.png)\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is set up with the hyperparameter below. The vocab size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "is applied to track the loss and\n",
    "`SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "implements stochastic gradient descent method as the optimizer. The initial\n",
    "learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "applied to adjust the learn rate through epochs. During the\n",
    "training, we use\n",
    "`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "function to scale all the gradient together to prevent exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 16.63s | valid loss  8.74 | valid ppl  6262.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 15.86s | valid loss  8.67 | valid ppl  5842.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 17.22s | valid loss  8.62 | valid ppl  5525.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 17.76s | valid loss  8.52 | valid ppl  4990.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 16.88s | valid loss  8.37 | valid ppl  4312.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 16.86s | valid loss  8.22 | valid ppl  3732.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 16.33s | valid loss  7.99 | valid ppl  2964.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 19.42s | valid loss  7.78 | valid ppl  2389.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 17.71s | valid loss  7.39 | valid ppl  1622.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 21.39s | valid loss  7.07 | valid ppl  1171.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 18.20s | valid loss  6.71 | valid ppl   820.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 16.74s | valid loss  6.35 | valid ppl   571.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 20.13s | valid loss  5.95 | valid ppl   384.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 19.47s | valid loss  5.69 | valid ppl   296.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 16.75s | valid loss  5.23 | valid ppl   187.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 17.91s | valid loss  4.89 | valid ppl   133.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 18.58s | valid loss  4.56 | valid ppl    95.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 21.25s | valid loss  4.26 | valid ppl    70.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 21.75s | valid loss  4.00 | valid ppl    54.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 20.25s | valid loss  3.73 | valid ppl    41.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 18.28s | valid loss  3.50 | valid ppl    33.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 21.33s | valid loss  3.29 | valid ppl    26.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 18.96s | valid loss  3.10 | valid ppl    22.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 17.80s | valid loss  2.94 | valid ppl    18.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 22.71s | valid loss  2.79 | valid ppl    16.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 22.80s | valid loss  2.64 | valid ppl    14.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 20.98s | valid loss  2.53 | valid ppl    12.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 18.18s | valid loss  2.42 | valid ppl    11.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 20.48s | valid loss  2.33 | valid ppl    10.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 22.53s | valid loss  2.24 | valid ppl     9.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 20.26s | valid loss  2.16 | valid ppl     8.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 25.09s | valid loss  2.09 | valid ppl     8.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 27.58s | valid loss  2.02 | valid ppl     7.57\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 28.01s | valid loss  1.96 | valid ppl     7.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 18.32s | valid loss  1.91 | valid ppl     6.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 26.69s | valid loss  1.86 | valid ppl     6.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 21.97s | valid loss  1.81 | valid ppl     6.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 21.51s | valid loss  1.77 | valid ppl     5.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 22.28s | valid loss  1.73 | valid ppl     5.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 24.58s | valid loss  1.70 | valid ppl     5.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 20.35s | valid loss  1.67 | valid ppl     5.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 23.13s | valid loss  1.63 | valid ppl     5.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 30.61s | valid loss  1.60 | valid ppl     4.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 28.06s | valid loss  1.58 | valid ppl     4.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 21.48s | valid loss  1.55 | valid ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 18.49s | valid loss  1.53 | valid ppl     4.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 22.67s | valid loss  1.51 | valid ppl     4.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 24.67s | valid loss  1.49 | valid ppl     4.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 21.67s | valid loss  1.47 | valid ppl     4.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 20.51s | valid loss  1.45 | valid ppl     4.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 21.04s | valid loss  1.44 | valid ppl     4.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 22.34s | valid loss  1.42 | valid ppl     4.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 24.23s | valid loss  1.41 | valid ppl     4.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 20.15s | valid loss  1.39 | valid ppl     4.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 20.21s | valid loss  1.38 | valid ppl     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 21.58s | valid loss  1.37 | valid ppl     3.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 23.25s | valid loss  1.36 | valid ppl     3.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 19.47s | valid loss  1.35 | valid ppl     3.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 23.67s | valid loss  1.34 | valid ppl     3.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 19.86s | valid loss  1.33 | valid ppl     3.77\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 60 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(13437, 200)\n",
       "  (decoder): Linear(in_features=200, out_features=13437, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = torch.transpose(train_data,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mot = train_data[:,0:1].clone()\\nmot\\nfor x in converter(mot):\\n    for i in x:\\n        print(i)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mot = train_data[:,0:1].clone()\n",
    "mot\n",
    "for x in converter(mot):\n",
    "    for i in x:\n",
    "        print(i)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = np.vectorize(lambda x: TEXT.vocab.itos[x])\n",
    "unconverter = np.vectorize(lambda x: TEXT.vocab.stoi[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([827, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mot = train_data[:,14:15].clone()\n",
    "mot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot = mot[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097+\n",
      "2912+\n",
      "3191+\n",
      "3409+\n",
      "3358-\n",
      "2444+\n",
      "2459+\n",
      "2610-\n",
      "2775+\n",
      "2338+\n",
      "1913+\n",
      "1855+\n",
      "1889-\n",
      "1822+\n",
      "1877+\n",
      "1680+\n",
      "1733+\n",
      "1668+\n",
      "1393+\n",
      "1262+\n",
      "1479+\n",
      "1434+\n",
      "996+\n",
      "794-\n",
      "955+\n",
      "904+\n",
      "829+\n",
      "715+\n",
      "518+\n",
      "470+\n",
      "384+\n",
      "203+\n",
      "80+\n",
      "66+\n",
      "200+\n",
      "5250+\n",
      "4192+\n",
      "4606+\n",
      "4068+\n",
      "3981+\n",
      "3391+\n",
      "3393+\n",
      "2034-\n",
      ".\n",
      "2822+\n",
      "910+\n",
      "328-\n",
      "2531-\n",
      "2802+\n",
      "2395-\n",
      "2144+\n",
      "2030+\n",
      "1873-\n",
      "855+\n",
      "2466+\n",
      "2257+\n",
      "2031+\n",
      "2013-\n",
      "2473+\n",
      "2262-\n",
      "2189+\n",
      "2083-\n",
      "2021-\n",
      "1831-\n",
      "190+\n",
      "2410+\n",
      "2042-\n",
      "1915-\n",
      "1959+\n",
      "831+\n",
      "2379+\n",
      "2224+\n",
      "2035+\n",
      "199+\n",
      "2391+\n",
      "2037+\n",
      "2775+\n",
      "2670+\n",
      "1945+\n",
      "1724+\n",
      "620-\n",
      "2803-\n",
      "2390+\n",
      "2380+\n",
      "1790-\n",
      "2696-\n",
      "2043-\n",
      "2419+\n",
      "1882-\n",
      "1871-\n",
      "1958-\n",
      "2019+\n",
      "1930+\n",
      "2324+\n",
      "2077+\n",
      "2027+\n",
      "1796+\n",
      "1795+\n",
      "2134-\n",
      "2029+\n",
      "1741-\n",
      "2832+\n",
      "1960+\n",
      "316-\n",
      "1289+\n",
      ".\n",
      "509-\n",
      "508+\n",
      "337-\n",
      "257+\n",
      "123-\n",
      "506+\n",
      "307+\n",
      "302+\n",
      "138-\n",
      "126-\n",
      "277+\n",
      "197-\n",
      "510+\n",
      "378-\n",
      "134-\n",
      "398+\n",
      "217-\n",
      "249+\n",
      "43+\n",
      "20-\n",
      "395+\n",
      "322+\n",
      "35-\n",
      "229-\n",
      "133-\n",
      "314+\n",
      "418-\n",
      "376-\n",
      "274-\n",
      "145-\n",
      "507+\n",
      "485+\n",
      "202-\n",
      "383-\n",
      "334-\n",
      "308-\n",
      "254-\n",
      "157-\n",
      "172-\n",
      "144-\n",
      "131-\n",
      "287-\n",
      "364-\n",
      "472-\n",
      "436+\n",
      "504+\n",
      "190+\n",
      "203-\n",
      "136-\n",
      "484-\n",
      "412+\n",
      "5-\n",
      "22-\n",
      "473-\n",
      "2+\n",
      "630-\n",
      ".\n",
      "510+\n",
      "0+\n",
      "418+\n",
      "314+\n",
      "1938+\n",
      "6388+\n",
      "733+\n",
      "277+\n",
      "913+\n",
      "921+\n",
      "895+\n",
      "4170+\n",
      "2224+\n",
      "694+\n",
      "257+\n",
      "902+\n",
      "602+\n",
      "461+\n",
      "659+\n",
      "898+\n",
      "6693+\n",
      "3617+\n",
      "3979+\n",
      "3136+\n",
      "8061+\n",
      "224+\n",
      "7022+\n",
      "8045+\n",
      "8267+\n",
      "897+\n",
      "6755+\n",
      "4791+\n",
      "46+\n",
      "3633+\n",
      "8449+\n",
      "8358+\n",
      "7354+\n",
      "6753+\n",
      "7644+\n",
      "7752+\n",
      "7670+\n",
      "7181+\n",
      "7163+\n",
      "7647+\n",
      "7723+\n",
      "6784+\n",
      "7521+\n",
      "7144-\n",
      "7171-\n",
      "6864+\n",
      "6712+\n",
      "7669+\n",
      "7679+\n",
      "7601-\n",
      "7346-\n",
      "6917+\n",
      "7569+\n",
      "7356+\n",
      "6943-\n",
      "7773+\n",
      "7757+\n",
      "7425+\n",
      "7446+\n",
      "7732+\n",
      "7094+\n",
      "7305-\n",
      "6852+\n",
      "7199-\n",
      "6931-\n",
      "7552-\n",
      "7479-\n",
      "7707+\n",
      "6706+\n",
      "7257-\n",
      "7725+\n",
      "7299+\n",
      "7404+\n",
      "7599+\n",
      "7585+\n",
      "6893-\n",
      "7402+\n",
      "6762-\n",
      "7761-\n",
      "7625-\n",
      "7372-\n",
      "7556+\n",
      "6652+\n",
      "97+\n",
      "123+\n",
      "910+\n",
      "899+\n",
      "508+\n",
      "520+\n",
      "302+\n",
      "1502+\n",
      "322-\n",
      "512-\n",
      "3189+\n",
      "989+\n",
      "1978+\n",
      "32+\n",
      "815+\n",
      "1283+\n",
      "504+\n",
      "472+\n",
      "2037+\n",
      "98+\n",
      "4421+\n",
      "706+\n",
      "1266+\n",
      "1796+\n",
      "3563+\n",
      "4354+\n",
      "2030+\n",
      "3814-\n",
      "2194+\n",
      "908+\n",
      "1444+\n",
      "33+\n",
      "4900+\n",
      "4607+\n",
      "1066+\n",
      "1+\n",
      "18+\n",
      "1703+\n",
      "1210+\n",
      "1627+\n",
      "483+\n",
      "2102+\n",
      "5363+\n",
      "2034+\n",
      "3006+\n",
      "2096+\n",
      "5901+\n",
      "92+\n",
      "355+\n",
      "295+\n",
      "2992+\n",
      "1521+\n",
      "3569-\n",
      "945+\n",
      "3827-\n",
      "2802+\n",
      "986+\n",
      "276+\n",
      "702+\n",
      "1644+\n",
      "4522+\n",
      "5250-\n",
      "486+\n",
      "1073+\n",
      "619-\n",
      "976+\n",
      "950+\n",
      "7355+\n",
      "1430+\n",
      "5150-\n",
      "2941+\n",
      "4070-\n",
      "792+\n",
      "785+\n",
      "2392+\n",
      "44+\n",
      "1480+\n",
      "513+\n",
      "131+\n",
      "1045+\n",
      "4+\n",
      "2324+\n",
      "5156-\n",
      "2109+\n",
      "980+\n",
      "62+\n",
      "1904+\n",
      "6726-\n",
      "5880-\n",
      "4007-\n",
      "1294+\n",
      "3740+\n",
      "551+\n",
      "620+\n",
      "6314+\n",
      "809+\n",
      "1123+\n",
      "2729+\n",
      "742+\n",
      "991+\n",
      "1705+\n",
      "1543+\n",
      "6621+\n",
      "1616+\n",
      "6596+\n",
      "3162+\n",
      "7195-\n",
      "823+\n",
      "3641+\n",
      "928+\n",
      "6045+\n",
      "6204+\n",
      "786+\n",
      "1468+\n",
      "5955+\n",
      "969+\n",
      "1401+\n",
      "1542+\n",
      "4350+\n",
      "184+\n",
      "2650+\n",
      "1051+\n",
      "1601+\n",
      "942+\n",
      "2076-\n",
      "527+\n",
      "1584+\n",
      "459+\n",
      "831+\n",
      "2637+\n",
      "4604+\n",
      "789+\n",
      "2390+\n",
      "1881+\n",
      "1374+\n",
      "7026+\n",
      "1549+\n",
      "2984+\n",
      "5722+\n",
      "3520+\n",
      "95+\n",
      "3086+\n",
      "2222+\n",
      "6058+\n",
      "1660+\n",
      "53+\n",
      "993+\n",
      "4135+\n",
      "1558+\n",
      "2220+\n",
      "5370+\n",
      "5163+\n",
      "6046+\n",
      "3465+\n",
      "3834+\n",
      "2999+\n",
      "6505-\n",
      "943+\n",
      "1822+\n",
      "1739+\n",
      "2578+\n",
      "7338-\n",
      "7197+\n",
      "2972+\n",
      "649+\n",
      "6059+\n",
      "783+\n",
      "2328+\n",
      "1785+\n",
      "1808+\n",
      "5777+\n",
      "2996+\n",
      "6471+\n",
      "2015-\n",
      "7396+\n",
      "17+\n",
      "1809+\n",
      "7675-\n",
      "5355+\n",
      "1176+\n",
      "5253+\n",
      "1810+\n",
      "7750+\n",
      "1275+\n",
      "4068+\n",
      "6563+\n",
      "4440+\n",
      "4416-\n",
      "5371+\n",
      "2285+\n",
      "6743-\n",
      "2739+\n",
      "3867+\n",
      "4633+\n",
      "6195+\n",
      "7061-\n",
      "3312+\n",
      "1190+\n",
      "5988+\n",
      "6737-\n",
      "7307-\n",
      "89+\n",
      "6227+\n",
      "7626-\n",
      "2789+\n",
      "790+\n",
      "3560+\n",
      "2041+\n",
      "7448-\n",
      "1281+\n",
      "2589+\n",
      "3140-\n",
      "1272+\n",
      "1035+\n",
      "773-\n",
      "7749+\n",
      "3241-\n",
      "4648+\n",
      "6629+\n",
      "1550+\n",
      "770+\n",
      "4383+\n",
      "5298+\n",
      "3694+\n",
      "5667+\n",
      "1526+\n",
      "812+\n",
      "3645+\n",
      "2727+\n",
      "7261-\n",
      "429+\n",
      "3553+\n",
      "7937+\n",
      "5939+\n",
      "8663+\n",
      "4131+\n",
      "4926+\n",
      "1297+\n",
      "827+\n",
      "8354+\n",
      "929+\n",
      "332+\n",
      "956+\n",
      "6697+\n",
      "7337-\n",
      "3539+\n",
      "883+\n",
      "8885+\n",
      "2601+\n",
      "1733+\n",
      "5990+\n",
      "2035+\n",
      "1437+\n",
      "8421+\n",
      "8572+\n",
      "8882+\n",
      "8817-\n",
      "9006+\n",
      "8976+\n",
      "6993+\n",
      "4010+\n",
      "15+\n",
      "239+\n",
      "2257+\n",
      "7452+\n",
      "9144+\n",
      "8665-\n",
      "922+\n",
      "906+\n",
      "2380+\n",
      "6631+\n",
      "8552+\n",
      "7966+\n",
      "6819+\n",
      "4904+\n",
      "6389+\n",
      "8286+\n",
      "4606+\n",
      "705+\n",
      "7591+\n",
      "6905+\n",
      "9444-\n",
      "325+\n",
      "6298+\n",
      "1820+\n",
      "7029+\n",
      "3428+\n",
      "3157+\n",
      "1421+\n",
      "1492+\n",
      ".\n",
      "1939+\n",
      "4229-\n",
      "314+\n",
      "696+\n",
      "973-\n",
      "968-\n",
      "944+\n",
      "1070-\n",
      "1182-\n",
      "4131+\n",
      "3633-\n",
      "3617+\n",
      "3470+\n",
      "3563+\n",
      "2996+\n",
      "2776-\n",
      "2543+\n",
      "3191+\n",
      "3152+\n",
      "2489-\n",
      "4155+\n",
      "4193+\n",
      "3785+\n",
      "4267+\n",
      "1796-\n",
      "867+\n",
      "599+\n",
      "963+\n",
      "923+\n",
      "35+\n",
      "2306+\n",
      "315+\n",
      "300+\n",
      "1231+\n",
      "2438+\n",
      "1149-\n",
      "472+\n",
      "325-\n",
      "436-\n",
      "465-\n",
      "2285+\n",
      "527-\n",
      "2248+\n",
      "1544+\n",
      "578-\n",
      "512+\n",
      "1756+\n",
      "1705-\n",
      "618-\n",
      "506+\n",
      "322+\n",
      "1272-\n",
      "683+\n",
      "2028-\n",
      "2284+\n",
      "1153-\n",
      "3898-\n",
      "2013-\n",
      "3303+\n",
      "514+\n",
      "2044+\n",
      "3782-\n",
      "3282+\n",
      "3441+\n",
      "3405-\n",
      "1869-\n",
      "1710-\n",
      "986+\n",
      "1423+\n",
      "511+\n",
      "1542+\n",
      "780+\n",
      "2066+\n",
      "826-\n",
      "3578-\n",
      "4177-\n",
      "1372-\n",
      "504-\n",
      "93+\n",
      "615+\n",
      "1974-\n",
      "9-\n",
      "945+\n",
      "560-\n",
      "3420+\n",
      "1510-\n",
      "1242-\n",
      "1046-\n",
      "1857-\n",
      "2100-\n",
      "2099-\n",
      "1264+\n",
      "906-\n",
      "951-\n",
      "78-\n",
      "1567-\n",
      "2006+\n",
      "1978-\n",
      "224-\n",
      "2131-\n",
      "1610-\n",
      "30+\n",
      "592-\n",
      "650+\n",
      "1083-\n",
      "3881-\n",
      "1176+\n",
      "920+\n",
      "1317+\n",
      "3569+\n",
      "1062+\n",
      "461+\n",
      "1823+\n",
      "4070+\n",
      "1177+\n",
      "3189+\n",
      "2604+\n",
      "3178+\n",
      "715+\n",
      "2998+\n",
      "1318+\n",
      "406+\n",
      "43-\n",
      "1473-\n",
      "380-\n",
      "659-\n",
      "1946+\n",
      "717+\n",
      "4450+\n",
      "3037+\n",
      "2962-\n",
      "2391+\n",
      "2802+\n",
      "4636+\n",
      "4354+\n",
      "3740+\n",
      "4460+\n",
      "4793+\n",
      "4707+\n",
      "4708-\n",
      "4677-\n",
      "4346+\n",
      "2019+\n",
      "3427-\n",
      "1346-\n",
      "3594-\n",
      "3469+\n",
      "5156+\n",
      "3979+\n",
      "4417+\n",
      "5293+\n",
      "701-\n",
      "337+\n",
      "97-\n",
      "2017+\n",
      "3185-\n",
      "3953-\n",
      "5320+\n",
      "450-\n",
      "5363+\n",
      "1922-\n",
      "3814+\n",
      "3829+\n",
      "254+\n",
      "4256+\n",
      "509-\n",
      "1-\n",
      "1882-\n",
      "4892+\n",
      "2383-\n",
      "3644+\n",
      "5722+\n",
      "5998+\n",
      "5667-\n",
      "5250+\n",
      "318-\n",
      "1601-\n",
      "1822-\n",
      "6045+\n",
      "3407-\n",
      "4136+\n",
      "2729+\n",
      "404+\n",
      "3935+\n",
      "3908+\n",
      "719-\n",
      "4522-\n",
      "4421-\n",
      "1730+\n",
      "6180+\n",
      "6505+\n",
      "4910-\n",
      "4167-\n",
      "6388+\n",
      "6453-\n",
      "6612+\n",
      "4900-\n",
      "5371+\n",
      "6058+\n",
      "6770+\n",
      "5885-\n",
      "6465+\n",
      "409+\n",
      "5819-\n",
      "4860+\n",
      "5420+\n",
      "2733+\n",
      "700+\n",
      "520+\n",
      "5319+\n",
      "7261-\n",
      "7195-\n",
      "862+\n",
      "5150+\n",
      "6469+\n",
      "4719+\n",
      "7197+\n",
      "6061+\n",
      "598-\n",
      "7403+\n",
      "6225+\n",
      "4606+\n",
      "649-\n",
      "5856+\n",
      "6314+\n",
      "4239-\n",
      "6438+\n",
      "3136+\n",
      "6489-\n",
      "6641+\n",
      "4914+\n",
      "3016+\n",
      "7676+\n",
      "6260+\n",
      "6630-\n",
      "7289+\n",
      "7399+\n",
      "5254-\n",
      "2912-\n",
      "2800-\n",
      "7749+\n",
      "7450+\n",
      "7190+\n",
      "7837+\n",
      "7869+\n",
      "7067-\n",
      "7686+\n",
      "7852+\n",
      "7543-\n",
      "7449+\n",
      "4551+\n",
      "6512+\n",
      "1543+\n",
      "6335+\n",
      "6471-\n",
      "6734+\n",
      "7847-\n",
      "5988+\n",
      "6530+\n",
      "6544+\n",
      "4060-\n",
      "7378-\n",
      "2871+\n",
      "7022+\n",
      "5217+\n",
      "7448+\n",
      "8329+\n",
      "8495+\n",
      "6621+\n",
      "3609+\n",
      "6276+\n",
      "7903-\n",
      "7285+\n",
      ".\n",
      "277+\n",
      "1729+\n",
      "461+\n",
      "2224+\n",
      "1938+\n",
      "7355+\n",
      "1733+\n",
      "6294+\n",
      "5324+\n",
      "6416+\n",
      "4786+\n",
      "6045-\n"
     ]
    }
   ],
   "source": [
    "for x in converter(mot):\n",
    "    for i in x:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2912+'] par ['2912+']\n",
      "['3191+'] par ['3191+']\n",
      "['3409+'] par ['3409+']\n",
      "['3358-'] par ['3358-']\n",
      "['2444+'] par ['3086+']\n",
      "['2459+'] par ['2459+']\n",
      "['2610-'] par ['2610-']\n",
      "['2775+'] par ['2775+']\n",
      "['2338+'] par ['2338+']\n",
      "['1913+'] par ['2325-']\n",
      "['1855+'] par ['1855+']\n",
      "['1889-'] par ['1889-']\n",
      "['1822+'] par ['1822+']\n",
      "['1877+'] par ['1877+']\n",
      "['1680+'] par ['1680+']\n",
      "['1733+'] par ['1733+']\n",
      "['1668+'] par ['1668+']\n",
      "['1393+'] par ['1393+']\n",
      "['1262+'] par ['1262+']\n",
      "['1479+'] par ['1479+']\n",
      "['1434+'] par ['6985+']\n",
      "['996+'] par ['4675-']\n",
      "['794-'] par ['794-']\n",
      "['955+'] par ['190+']\n",
      "['904+'] par ['609-']\n",
      "['829+'] par ['829+']\n",
      "['715+'] par ['715+']\n",
      "['518+'] par ['518+']\n",
      "['470+'] par ['470+']\n",
      "['384+'] par ['384+']\n",
      "['203+'] par ['2038-']\n",
      "['80+'] par ['80+']\n",
      "['66+'] par ['66+']\n",
      "['200+'] par ['200+']\n",
      "['5250+'] par ['5250+']\n",
      "['4192+'] par ['4352-']\n",
      "['4606+'] par ['3564+']\n",
      "['4068+'] par ['649-']\n",
      "['3981+'] par ['6155-']\n",
      "['3391+'] par ['3391+']\n",
      "['3393+'] par ['3393+']\n",
      "['2034-'] par ['263+']\n",
      "['.'] par ['4853+']\n",
      "['2822+'] par ['509-']\n",
      "['910+'] par ['910+']\n",
      "['328-'] par ['1750-']\n",
      "['2531-'] par ['2531-']\n",
      "['2802+'] par ['1365+']\n",
      "['2395-'] par ['1153+']\n",
      "['2144+'] par ['1382-']\n",
      "['2030+'] par ['2133-']\n",
      "['1873-'] par ['3454+']\n",
      "['855+'] par ['855+']\n",
      "['2466+'] par ['2231-']\n",
      "['2257+'] par ['1431+']\n",
      "['2031+'] par ['2731+']\n",
      "['2013-'] par ['2013-']\n",
      "['2473+'] par ['15-']\n",
      "['2262-'] par ['8434+']\n",
      "['2189+'] par ['797-']\n",
      "['2083-'] par ['163-']\n",
      "['2021-'] par ['1480-']\n",
      "['1831-'] par ['2369+']\n",
      "['190+'] par ['190+']\n",
      "['2410+'] par ['37-']\n",
      "['2042-'] par ['1770-']\n",
      "['1915-'] par ['1915-']\n",
      "['1959+'] par ['1754-']\n",
      "['831+'] par ['1542+']\n",
      "['2379+'] par ['464-']\n",
      "['2224+'] par ['6192-']\n",
      "['2035+'] par ['2670+']\n",
      "['199+'] par ['1754-']\n",
      "['2391+'] par ['2426-']\n",
      "['2037+'] par ['2030+']\n",
      "['2775+'] par ['1807-']\n",
      "['2670+'] par ['2803-']\n",
      "['1945+'] par ['2620+']\n",
      "['1724+'] par ['4610+']\n",
      "['620-'] par ['7465+']\n",
      "['2803-'] par ['2803-']\n",
      "['2390+'] par ['2884-']\n",
      "['2380+'] par ['303+']\n",
      "['1790-'] par ['2654+']\n",
      "['2696-'] par ['2696-']\n",
      "['2043-'] par ['2370+']\n",
      "['2419+'] par ['2650+']\n",
      "['1882-'] par ['1882-']\n",
      "['1871-'] par ['2030+']\n",
      "['1958-'] par ['1836-']\n",
      "['2019+'] par ['1355+']\n",
      "['1930+'] par ['483+']\n",
      "['2324+'] par ['2324+']\n",
      "['2077+'] par ['2006-']\n",
      "['2027+'] par ['2030+']\n",
      "['1796+'] par ['1290+']\n",
      "['1795+'] par ['2638+']\n",
      "['2134-'] par ['2761+']\n",
      "['2029+'] par ['2030+']\n",
      "['1741-'] par ['1945+']\n",
      "['2832+'] par ['1960+']\n",
      "['1960+'] par ['2979+']\n",
      "['316-'] par ['1839+']\n",
      "['1289+'] par ['1289+']\n",
      "['.'] par ['1229+']\n",
      "['509-'] par ['1290+']\n",
      "['508+'] par ['508-']\n",
      "['337-'] par ['337+']\n",
      "['257+'] par ['257+']\n",
      "['123-'] par ['1881+']\n",
      "['506+'] par ['5172-']\n",
      "['307+'] par ['307+']\n",
      "['302+'] par ['302+']\n",
      "['138-'] par ['3306-']\n",
      "['126-'] par ['31-']\n",
      "['277+'] par ['297-']\n",
      "['197-'] par ['1283+']\n",
      "['510+'] par ['418-']\n",
      "['378-'] par ['1283+']\n",
      "['134-'] par ['116-']\n",
      "['398+'] par ['133-']\n",
      "['217-'] par ['314-']\n",
      "['249+'] par ['314-']\n",
      "['43+'] par ['24+']\n",
      "['20-'] par ['483+']\n",
      "['395+'] par ['10-']\n",
      "['322+'] par ['450+']\n",
      "['35-'] par ['1586-']\n",
      "['229-'] par ['2013+']\n",
      "['133-'] par ['2030+']\n",
      "['314+'] par ['35-']\n",
      "['418-'] par ['1290+']\n",
      "['376-'] par ['6429+']\n",
      "['274-'] par ['2030+']\n",
      "['145-'] par ['314-']\n",
      "['507+'] par ['1080-']\n",
      "['485+'] par ['307+']\n",
      "['202-'] par ['1266+']\n",
      "['383-'] par ['10-']\n",
      "['334-'] par ['1153+']\n",
      "['308-'] par ['1543-']\n",
      "['254-'] par ['244+']\n",
      "['157-'] par ['2660-']\n",
      "['172-'] par ['1080-']\n",
      "['144-'] par ['5987-']\n",
      "['131-'] par ['406-']\n",
      "['287-'] par ['24+']\n",
      "['364-'] par ['35-']\n",
      "['472-'] par ['4522+']\n",
      "['436+'] par ['23-']\n",
      "['504+'] par ['1572+']\n",
      "['190+'] par ['2013-']\n",
      "['203-'] par ['559+']\n",
      "['136-'] par ['133+']\n",
      "['484-'] par ['484-']\n",
      "['412+'] par ['1398-']\n",
      "['5-'] par ['921+']\n",
      "['22-'] par ['1791+']\n",
      "['473-'] par ['19+']\n",
      "['2+'] par ['3-']\n",
      "['630-'] par ['622+']\n",
      "['.'] par ['191+']\n",
      "['510+'] par ['509+']\n",
      "['0+'] par ['418+']\n",
      "['418+'] par ['406-']\n",
      "['314+'] par ['1397+']\n",
      "['1938+'] par ['418-']\n",
      "['6388+'] par ['55+']\n",
      "['733+'] par ['322+']\n",
      "['277+'] par ['991+']\n",
      "['913+'] par ['234+']\n",
      "['921+'] par ['2013+']\n",
      "['895+'] par ['1497-']\n",
      "['4170+'] par ['436+']\n",
      "['2224+'] par ['3756-']\n",
      "['694+'] par ['1444-']\n",
      "['257+'] par ['3644-']\n",
      "['902+'] par ['436+']\n",
      "['602+'] par ['32+']\n",
      "['461+'] par ['35-']\n",
      "['659+'] par ['32+']\n",
      "['898+'] par ['838+']\n",
      "['6693+'] par ['55+']\n",
      "['3617+'] par ['138+']\n",
      "['3979+'] par ['862+']\n",
      "['3136+'] par ['4145+']\n",
      "['8061+'] par ['2248+']\n",
      "['224+'] par ['5363+']\n",
      "['7022+'] par ['138+']\n",
      "['8045+'] par ['4354+']\n",
      "['8267+'] par ['4522+']\n",
      "['897+'] par ['3635+']\n",
      "['6755+'] par ['472-']\n",
      "['4791+'] par ['613+']\n",
      "['46+'] par ['334+']\n",
      "['3633+'] par ['20+']\n",
      "['8449+'] par ['4791+']\n",
      "['8358+'] par ['1575+']\n",
      "['7354+'] par ['43+']\n",
      "['6753+'] par ['5324+']\n",
      "['7644+'] par ['6576-']\n",
      "['7752+'] par ['2133-']\n",
      "['7670+'] par ['1297-']\n",
      "['7181+'] par ['1180+']\n",
      "['7163+'] par ['1321+']\n",
      "['7647+'] par ['6753-']\n",
      "['7723+'] par ['9053+']\n",
      "['6784+'] par ['7888-']\n",
      "['7521+'] par ['5979+']\n",
      "['7144-'] par ['512-']\n",
      "['7171-'] par ['476-']\n",
      "['6864+'] par ['1601+']\n",
      "['6712+'] par ['963+']\n",
      "['7669+'] par ['6256-']\n",
      "['7679+'] par ['1231+']\n",
      "['7601-'] par ['7601-']\n",
      "['7346-'] par ['6241-']\n",
      "['6917+'] par ['825+']\n",
      "['7569+'] par ['6926-']\n",
      "['7356+'] par ['5653+']\n",
      "['6943-'] par ['6943-']\n",
      "['7773+'] par ['5161-']\n",
      "['7757+'] par ['7757+']\n",
      "['7425+'] par ['6965+']\n",
      "['7446+'] par ['7283-']\n",
      "['7732+'] par ['506+']\n",
      "['7094+'] par ['7094+']\n",
      "['7305-'] par ['7034+']\n",
      "['6852+'] par ['1230+']\n",
      "['7199-'] par ['7199-']\n",
      "['6931-'] par ['832+']\n",
      "['7552-'] par ['7552-']\n",
      "['7479-'] par ['7713+']\n",
      "['7707+'] par ['7619-']\n",
      "['6706+'] par ['7539+']\n",
      "['7257-'] par ['968+']\n",
      "['7725+'] par ['7646-']\n",
      "['7299+'] par ['594-']\n",
      "['7404+'] par ['7404+']\n",
      "['7599+'] par ['1670-']\n",
      "['7585+'] par ['7585+']\n",
      "['6893-'] par ['234+']\n",
      "['7402+'] par ['7402+']\n",
      "['6762-'] par ['6762-']\n",
      "['7761-'] par ['7761-']\n",
      "['7625-'] par ['832+']\n",
      "['7372-'] par ['2013-']\n",
      "['7556+'] par ['7556+']\n",
      "['6652+'] par ['23-']\n",
      "['97+'] par ['97+']\n",
      "['123+'] par ['0+']\n",
      "['910+'] par ['878+']\n",
      "['899+'] par ['1767+']\n",
      "['508+'] par ['898+']\n",
      "['520+'] par ['461+']\n",
      "['302+'] par ['46+']\n",
      "['1502+'] par ['138+']\n",
      "['322-'] par ['585+']\n",
      "['512-'] par ['512-']\n",
      "['3189+'] par ['302+']\n",
      "['989+'] par ['302+']\n",
      "['1978+'] par ['910+']\n",
      "['32+'] par ['32+']\n",
      "['815+'] par ['20+']\n",
      "['1283+'] par ['1283+']\n",
      "['504+'] par ['1153+']\n",
      "['472+'] par ['1283+']\n",
      "['2037+'] par ['508+']\n",
      "['98+'] par ['1283+']\n",
      "['4421+'] par ['602+']\n",
      "['706+'] par ['3563+']\n",
      "['1266+'] par ['1266+']\n",
      "['1796+'] par ['24+']\n",
      "['3563+'] par ['1266+']\n",
      "['4354+'] par ['234+']\n",
      "['2030+'] par ['2077-']\n",
      "['3814-'] par ['2370+']\n",
      "['2194+'] par ['508+']\n",
      "['908+'] par ['6444+']\n",
      "['1444+'] par ['508+']\n",
      "['33+'] par ['15+']\n",
      "['4900+'] par ['15+']\n",
      "['4607+'] par ['968+']\n",
      "['1066+'] par ['4926+']\n",
      "['1+'] par ['5363+']\n",
      "['18+'] par ['3280+']\n",
      "['1703+'] par ['315+']\n",
      "['1210+'] par ['968+']\n",
      "['1627+'] par ['1756-']\n",
      "['483+'] par ['337+']\n",
      "['2102+'] par ['0+']\n",
      "['5363+'] par ['1479-']\n",
      "['2034+'] par ['6512+']\n",
      "['3006+'] par ['2031+']\n",
      "['2096+'] par ['2370+']\n",
      "['5901+'] par ['2370+']\n",
      "['92+'] par ['6512+']\n",
      "['355+'] par ['485+']\n",
      "['295+'] par ['32+']\n",
      "['2992+'] par ['291+']\n",
      "['1521+'] par ['2799-']\n",
      "['3569-'] par ['622+']\n",
      "['945+'] par ['485+']\n",
      "['3827-'] par ['2386+']\n",
      "['2802+'] par ['1978-']\n",
      "['986+'] par ['2638+']\n",
      "['276+'] par ['1959-']\n",
      "['702+'] par ['1281+']\n",
      "['1644+'] par ['1266+']\n",
      "['4522+'] par ['1701+']\n",
      "['5250-'] par ['2066+']\n",
      "['486+'] par ['2670+']\n",
      "['1073+'] par ['1767+']\n",
      "['619-'] par ['838+']\n",
      "['976+'] par ['2370+']\n",
      "['950+'] par ['254+']\n",
      "['7355+'] par ['1283+']\n",
      "['1430+'] par ['2670+']\n",
      "['5150-'] par ['1283+']\n",
      "['2941+'] par ['3189+']\n",
      "['4070-'] par ['3563-']\n",
      "['792+'] par ['1290+']\n",
      "['785+'] par ['2670-']\n",
      "['2392+'] par ['781-']\n",
      "['44+'] par ['2030+']\n",
      "['1480+'] par ['1480+']\n",
      "['513+'] par ['62+']\n",
      "['131+'] par ['2620-']\n",
      "['1045+'] par ['291+']\n",
      "['4+'] par ['164+']\n",
      "['2324+'] par ['1+']\n",
      "['5156-'] par ['1266+']\n",
      "['2109+'] par ['3456-']\n",
      "['980+'] par ['254+']\n",
      "['62+'] par ['512+']\n",
      "['1904+'] par ['6+']\n",
      "['6726-'] par ['6556+']\n",
      "['5880-'] par ['1266+']\n",
      "['4007-'] par ['3152+']\n",
      "['1294+'] par ['1321-']\n",
      "['3740+'] par ['1266+']\n",
      "['551+'] par ['422-']\n",
      "['620+'] par ['511-']\n",
      "['6314+'] par ['905-']\n",
      "['809+'] par ['1266+']\n",
      "['1123+'] par ['849+']\n",
      "['2729+'] par ['134-']\n",
      "['742+'] par ['2066+']\n",
      "['991+'] par ['508+']\n",
      "['1705+'] par ['461+']\n",
      "['1543+'] par ['503-']\n",
      "['6621+'] par ['2370+']\n",
      "['1616+'] par ['2173+']\n",
      "['6596+'] par ['220-']\n",
      "['3162+'] par ['257+']\n",
      "['7195-'] par ['2702+']\n",
      "['823+'] par ['3635-']\n",
      "['3641+'] par ['1791+']\n",
      "['928+'] par ['372+']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6045+'] par ['3162+']\n",
      "['6204+'] par ['4354-']\n",
      "['786+'] par ['1938+']\n",
      "['1468+'] par ['1720+']\n",
      "['5955+'] par ['994+']\n",
      "['969+'] par ['10+']\n",
      "['1401+'] par ['1211+']\n",
      "['1542+'] par ['2391+']\n",
      "['4350+'] par ['506+']\n",
      "['184+'] par ['3409+']\n",
      "['2650+'] par ['503+']\n",
      "['1051+'] par ['2290+']\n",
      "['1601+'] par ['2903+']\n",
      "['942+'] par ['784+']\n",
      "['2076-'] par ['1882+']\n",
      "['527+'] par ['942+']\n",
      "['1584+'] par ['3225+']\n",
      "['459+'] par ['.']\n",
      "['831+'] par ['409+']\n",
      "['2637+'] par ['777-']\n",
      "['4604+'] par ['1266+']\n",
      "['789+'] par ['2523+']\n",
      "['2390+'] par ['2435-']\n",
      "['1881+'] par ['302+']\n",
      "['1374+'] par ['2248-']\n",
      "['7026+'] par ['23+']\n",
      "['1549+'] par ['1032+']\n",
      "['2984+'] par ['1548+']\n",
      "['5722+'] par ['594-']\n",
      "['3520+'] par ['740+']\n",
      "['95+'] par ['3635+']\n",
      "['3086+'] par ['1176+']\n",
      "['2222+'] par ['2992+']\n",
      "['6058+'] par ['3241+']\n",
      "['1660+'] par ['443+']\n",
      "['53+'] par ['2903+']\n",
      "['993+'] par ['300+']\n",
      "['4135+'] par ['285+']\n",
      "['1558+'] par ['811+']\n",
      "['2220+'] par ['4791+']\n",
      "['5370+'] par ['1767+']\n",
      "['5163+'] par ['385+']\n",
      "['6046+'] par ['1882+']\n",
      "['3465+'] par ['898+']\n",
      "['3834+'] par ['2173+']\n",
      "['2999+'] par ['1082+']\n",
      "['6505-'] par ['2888-']\n",
      "['943+'] par ['7195-']\n",
      "['1822+'] par ['1822+']\n",
      "['1739+'] par ['1283+']\n",
      "['2578+'] par ['7241+']\n",
      "['7338-'] par ['2013+']\n",
      "['7197+'] par ['3002+']\n",
      "['2972+'] par ['700+']\n",
      "['649+'] par ['649+']\n",
      "['6059+'] par ['2191-']\n",
      "['783+'] par ['1971+']\n",
      "['2328+'] par ['4791+']\n",
      "['1785+'] par ['2799+']\n",
      "['1808+'] par ['975+']\n",
      "['5777+'] par ['2345+']\n",
      "['2996+'] par ['968+']\n",
      "['6471+'] par ['991+']\n",
      "['2015-'] par ['1971+']\n",
      "['7396+'] par ['506+']\n",
      "['17+'] par ['3635+']\n",
      "['1809+'] par ['62+']\n",
      "['7675-'] par ['968+']\n",
      "['5355+'] par ['3006-']\n",
      "['1176+'] par ['383+']\n",
      "['5253+'] par ['920+']\n",
      "['1810+'] par ['4302-']\n",
      "['7750+'] par ['16+']\n",
      "['1275+'] par ['418-']\n",
      "['4068+'] par ['2670+']\n",
      "['6563+'] par ['291+']\n",
      "['4440+'] par ['1819-']\n",
      "['4416-'] par ['1881+']\n",
      "['5371+'] par ['2995-']\n",
      "['2285+'] par ['62+']\n",
      "['6743-'] par ['322+']\n",
      "['2739+'] par ['5722-']\n",
      "['3867+'] par ['902+']\n",
      "['4633+'] par ['3873-']\n",
      "['6195+'] par ['6480+']\n",
      "['7061-'] par ['461+']\n",
      "['3312+'] par ['8247-']\n",
      "['1190+'] par ['2290+']\n",
      "['5988+'] par ['1281+']\n",
      "['6737-'] par ['2446+']\n",
      "['7307-'] par ['7325-']\n",
      "['89+'] par ['89+']\n",
      "['6227+'] par ['1297+']\n",
      "['7626-'] par ['5682+']\n",
      "['2789+'] par ['2789+']\n",
      "['790+'] par ['1502+']\n",
      "['3560+'] par ['1483-']\n",
      "['2041+'] par ['1297+']\n",
      "['7448-'] par ['3635+']\n",
      "['1281+'] par ['2144-']\n",
      "['2589+'] par ['2039+']\n",
      "['3140-'] par ['6631+']\n",
      "['1272+'] par ['3951+']\n",
      "['1035+'] par ['968+']\n",
      "['773-'] par ['860+']\n",
      "['7749+'] par ['592+']\n",
      "['3241-'] par ['8277+']\n",
      "['4648+'] par ['8219-']\n",
      "['6629+'] par ['3872-']\n",
      "['1550+'] par ['2620-']\n",
      "['770+'] par ['134-']\n",
      "['4383+'] par ['2486+']\n",
      "['5298+'] par ['7995+']\n",
      "['3694+'] par ['3785+']\n",
      "['5667+'] par ['10+']\n",
      "['1526+'] par ['3671+']\n",
      "['812+'] par ['812+']\n",
      "['3645+'] par ['1101-']\n",
      "['2727+'] par ['4153+']\n",
      "['7261-'] par ['1266+']\n",
      "['429+'] par ['429+']\n",
      "['3553+'] par ['1297+']\n",
      "['7937+'] par ['3827+']\n",
      "['5939+'] par ['8293+']\n",
      "['8663+'] par ['520+']\n",
      "['4131+'] par ['897+']\n",
      "['4926+'] par ['4791+']\n",
      "['1297+'] par ['2324+']\n",
      "['827+'] par ['968+']\n",
      "['8354+'] par ['7582+']\n",
      "['929+'] par ['963+']\n",
      "['332+'] par ['921+']\n",
      "['956+'] par ['2324+']\n",
      "['6697+'] par ['43+']\n",
      "['7337-'] par ['8711+']\n",
      "['3539+'] par ['6671+']\n",
      "['883+'] par ['3623+']\n",
      "['8885+'] par ['4070+']\n",
      "['2601+'] par ['2601+']\n",
      "['1733+'] par ['2290+']\n",
      "['5990+'] par ['1686+']\n",
      "['2035+'] par ['1602+']\n",
      "['1437+'] par ['3635+']\n",
      "['8421+'] par ['968+']\n",
      "['8572+'] par ['5212-']\n",
      "['8882+'] par ['6416+']\n",
      "['8817-'] par ['2224-']\n",
      "['9006+'] par ['7870-']\n",
      "['8976+'] par ['15+']\n",
      "['6993+'] par ['9445+']\n",
      "['4010+'] par ['1686+']\n",
      "['15+'] par ['1804-']\n",
      "['239+'] par ['3012+']\n",
      "['2257+'] par ['234+']\n",
      "['7452+'] par ['3849+']\n",
      "['9144+'] par ['254-']\n",
      "['8665-'] par ['1266+']\n",
      "['922+'] par ['7602-']\n",
      "['906+'] par ['1494-']\n",
      "['2380+'] par ['613+']\n",
      "['6631+'] par ['1274+']\n",
      "['8552+'] par ['1251+']\n",
      "['7966+'] par ['1480+']\n",
      "['6819+'] par ['5260-']\n",
      "['4904+'] par ['6241+']\n",
      "['6389+'] par ['975+']\n",
      "['8286+'] par ['183+']\n",
      "['4606+'] par ['613+']\n",
      "['705+'] par ['418-']\n",
      "['7591+'] par ['696+']\n",
      "['6905+'] par ['2030+']\n",
      "['9444-'] par ['1436+']\n",
      "['325+'] par ['9600+']\n",
      "['6298+'] par ['898+']\n",
      "['1820+'] par ['4131+']\n",
      "['7029+'] par ['3563+']\n",
      "['3428+'] par ['2030+']\n",
      "['3157+'] par ['3204+']\n",
      "['1421+'] par ['8256+']\n",
      "['1492+'] par ['254+']\n",
      "['.'] par ['2533+']\n",
      "['1939+'] par ['509-']\n",
      "['4229-'] par ['254+']\n",
      "['314+'] par ['314+']\n",
      "['696+'] par ['1938+']\n",
      "['973-'] par ['506+']\n",
      "['968-'] par ['2992+']\n",
      "['944+'] par ['314-']\n",
      "['1070-'] par ['1266+']\n",
      "['1182-'] par ['3563+']\n",
      "['4131+'] par ['1290+']\n",
      "['3633-'] par ['4791+']\n",
      "['3617+'] par ['3623-']\n",
      "['3470+'] par ['3563+']\n",
      "['3563+'] par ['3563+']\n",
      "['2996+'] par ['62+']\n",
      "['2776-'] par ['991+']\n",
      "['2543+'] par ['898+']\n",
      "['3191+'] par ['4522+']\n",
      "['3152+'] par ['696+']\n",
      "['2489-'] par ['3880-']\n",
      "['4155+'] par ['1444+']\n",
      "['4193+'] par ['5682+']\n",
      "['3785+'] par ['3785+']\n",
      "['4267+'] par ['838+']\n",
      "['1796-'] par ['1796-']\n",
      "['867+'] par ['1321+']\n",
      "['599+'] par ['599-']\n",
      "['963+'] par ['1476-']\n",
      "['923+'] par ['1321+']\n",
      "['35+'] par ['585+']\n",
      "['2306+'] par ['968+']\n",
      "['315+'] par ['658+']\n",
      "['300+'] par ['898+']\n",
      "['1231+'] par ['508+']\n",
      "['2438+'] par ['1476-']\n",
      "['1149-'] par ['545-']\n",
      "['472+'] par ['621-']\n",
      "['325-'] par ['461+']\n",
      "['436-'] par ['1266+']\n",
      "['465-'] par ['835+']\n",
      "['2285+'] par ['4900+']\n",
      "['527-'] par ['2996+']\n",
      "['2248+'] par ['5291+']\n",
      "['1544+'] par ['613+']\n",
      "['578-'] par ['5901-']\n",
      "['512+'] par ['512+']\n",
      "['1756+'] par ['511-']\n",
      "['1705-'] par ['508+']\n",
      "['618-'] par ['618-']\n",
      "['506+'] par ['506+']\n",
      "['322+'] par ['334+']\n",
      "['1272-'] par ['1904+']\n",
      "['683+'] par ['618+']\n",
      "['2028-'] par ['508+']\n",
      "['2284+'] par ['291+']\n",
      "['1153-'] par ['508+']\n",
      "['3898-'] par ['2979+']\n",
      "['2013-'] par ['1153+']\n",
      "['3303+'] par ['2029+']\n",
      "['514+'] par ['514+']\n",
      "['2044+'] par ['302+']\n",
      "['3782-'] par ['1266-']\n",
      "['3282+'] par ['3282+']\n",
      "['3441+'] par ['1134+']\n",
      "['3405-'] par ['2467-']\n",
      "['1869-'] par ['.']\n",
      "['1710-'] par ['3635+']\n",
      "['986+'] par ['986+']\n",
      "['1423+'] par ['3635+']\n",
      "['511+'] par ['693+']\n",
      "['1542+'] par ['32+']\n",
      "['780+'] par ['6429+']\n",
      "['2066+'] par ['1290+']\n",
      "['826-'] par ['1443+']\n",
      "['3578-'] par ['4926+']\n",
      "['4177-'] par ['134+']\n",
      "['1372-'] par ['613+']\n",
      "['504-'] par ['2550-']\n",
      "['93+'] par ['2030+']\n",
      "['615+'] par ['615+']\n",
      "['1974-'] par ['508+']\n",
      "['9-'] par ['9-']\n",
      "['945+'] par ['418-']\n",
      "['560-'] par ['898+']\n",
      "['3420+'] par ['546-']\n",
      "['1510-'] par ['3563+']\n",
      "['1242-'] par ['1242-']\n",
      "['1046-'] par ['379-']\n",
      "['1857-'] par ['558+']\n",
      "['2100-'] par ['297-']\n",
      "['2099-'] par ['6389-']\n",
      "['1264+'] par ['2055-']\n",
      "['906-'] par ['705+']\n",
      "['951-'] par ['6241-']\n",
      "['78-'] par ['906+']\n",
      "['1567-'] par ['1701+']\n",
      "['2006+'] par ['1260+']\n",
      "['1978-'] par ['4136+']\n",
      "['224-'] par ['5880-']\n",
      "['2131-'] par ['897-']\n",
      "['1610-'] par ['24+']\n",
      "['30+'] par ['133-']\n",
      "['592-'] par ['40-']\n",
      "['650+'] par ['2173+']\n",
      "['1083-'] par ['472-']\n",
      "['3881-'] par ['1149+']\n",
      "['1176+'] par ['2996+']\n",
      "['920+'] par ['5253+']\n",
      "['1317+'] par ['138+']\n",
      "['3569+'] par ['3569+']\n",
      "['1062+'] par ['4070+']\n",
      "['461+'] par ['461+']\n",
      "['1823+'] par ['7355+']\n",
      "['4070+'] par ['2824+']\n",
      "['1177+'] par ['963+']\n",
      "['3189+'] par ['809+']\n",
      "['2604+'] par ['4791+']\n",
      "['3178+'] par ['6241+']\n",
      "['715+'] par ['715+']\n",
      "['2998+'] par ['4956+']\n",
      "['1318+'] par ['3135+']\n",
      "['406+'] par ['898+']\n",
      "['43-'] par ['117+']\n",
      "['1473-'] par ['418-']\n",
      "['380-'] par ['6259+']\n",
      "['659-'] par ['659-']\n",
      "['1946+'] par ['461-']\n",
      "['717+'] par ['2687+']\n",
      "['4450+'] par ['6360+']\n",
      "['3037+'] par ['2804-']\n",
      "['2962-'] par ['693+']\n",
      "['2391+'] par ['508+']\n",
      "['2802+'] par ['2192+']\n",
      "['4636+'] par ['1756+']\n",
      "['4354+'] par ['461+']\n",
      "['3740+'] par ['461+']\n",
      "['4460+'] par ['2940+']\n",
      "['4793+'] par ['2133-']\n",
      "['4707+'] par ['1521+']\n",
      "['4708-'] par ['4708-']\n",
      "['4677-'] par ['302+']\n",
      "['4346+'] par ['4346+']\n",
      "['2019+'] par ['2019+']\n",
      "['3427-'] par ['62+']\n",
      "['1346-'] par ['3319+']\n",
      "['3594-'] par ['897+']\n",
      "['3469+'] par ['281+']\n",
      "['5156+'] par ['5156+']\n",
      "['3979+'] par ['4070+']\n",
      "['4417+'] par ['4522+']\n",
      "['5293+'] par ['337-']\n",
      "['701-'] par ['701-']\n",
      "['337+'] par ['2194-']\n",
      "['97-'] par ['1044+']\n",
      "['2017+'] par ['1173-']\n",
      "['3185-'] par ['1177-']\n",
      "['3953-'] par ['3442+']\n",
      "['5320+'] par ['1044+']\n",
      "['450-'] par ['450-']\n",
      "['5363+'] par ['325+']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,700):\n",
    "    #print(\"mot\",converter(mot))\n",
    "    prediction = best_model(mot).argmax(axis=2)\n",
    "    print(converter(mot)[i + 1], 'par', converter(prediction)[ i])\n",
    "    #mot[ i+1] = prediction[ i]\n",
    "    #print(\"pred\" ,converter(prediction))\n",
    "    #print(\"mot\",converter(mot))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "test_sequences_list = []\n",
    "\n",
    "for i in test_dict.keys() :\n",
    "    for (movie, rating, t) in test_dict[i]:\n",
    "        if rating > 3.0:\n",
    "            test_sequences_list.append(str(movie) + \"+\")\n",
    "        else :\n",
    "            test_sequences_list.append(str(movie) + \"-\")\n",
    "    test_sequences_list.append(\".\")\n",
    "    \n",
    "manual_test_data = batchify(test_sequences_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mot = torch.flatten(manual_test_data.clone()).unsqueeze(-1)\n",
    "test_mot = test_mot[0:200,:]\n",
    "test_mot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277-\n",
      "320-\n",
      "<unk>\n",
      "2610+\n",
      "910+\n",
      "2877+\n",
      "443-\n",
      "470-\n",
      "1985+\n",
      "43+\n",
      "1398+\n",
      "1938+\n",
      "1644+\n",
      "1585+\n",
      "6405+\n",
      "994-\n",
      "2108-\n",
      "7493+\n",
      "3617+\n",
      "2048-\n",
      "904+\n",
      "1437+\n",
      "4402+\n",
      "7569+\n",
      "5686+\n",
      "897+\n",
      "1690+\n",
      "3404-\n",
      ".\n",
      "900+\n",
      "2588+\n",
      "2803+\n",
      "6241+\n",
      "6520+\n",
      "948+\n",
      "276+\n",
      "4131+\n",
      "1182-\n",
      "138-\n",
      "1059-\n",
      "1971-\n",
      "3814+\n",
      "3543-\n",
      "166-\n",
      "3910+\n",
      "6429-\n",
      "4145+\n",
      "6994+\n",
      "6901-\n",
      "2512+\n",
      "5150+\n",
      "2700+\n",
      "123-\n",
      "199+\n",
      "2257+\n",
      "2625+\n",
      "989+\n",
      "3189+\n",
      "3845+\n",
      "584-\n",
      "4007+\n",
      "455-\n",
      "6405+\n",
      "314-\n",
      "6405+\n",
      "1208-\n",
      "986+\n",
      "302-\n",
      "<unk>\n",
      "<unk>\n",
      "4135+\n",
      "<unk>\n",
      "4450+\n",
      "<unk>\n",
      "<unk>\n",
      "1635+\n",
      "6423+\n",
      "6517+\n",
      "3002+\n",
      "1776+\n",
      "3460+\n",
      "2592-\n",
      "3623+\n",
      "4789+\n",
      "1996-\n",
      "398+\n",
      "6544-\n",
      "3814+\n",
      "95-\n",
      "6225-\n",
      "6074-\n",
      "157+\n",
      "908-\n",
      "6465+\n",
      "1986+\n",
      "2315+\n",
      "4-\n",
      "2514+\n",
      "190+\n",
      "3635+\n",
      "7355+\n",
      "186-\n",
      "751+\n",
      "9203+\n",
      "508+\n",
      "3007+\n",
      "483+\n",
      "436-\n",
      "287+\n",
      "4328+\n",
      "1428+\n",
      "224+\n",
      "5895-\n",
      "3867+\n",
      "5938+\n",
      "1822+\n",
      "1810-\n",
      "116-\n",
      "694+\n",
      "1035-\n",
      "887+\n",
      "512+\n",
      "1916+\n",
      "7086+\n",
      "963+\n",
      "2654+\n",
      "1307+\n",
      "<unk>\n",
      "509-\n",
      "1041+\n",
      "2593+\n",
      "2390-\n",
      "92+\n",
      "5869+\n",
      "1223-\n",
      "133+\n",
      "510+\n",
      "3136+\n",
      "4900+\n",
      "1057+\n",
      "2067-\n",
      "2391+\n",
      "3123+\n",
      "124-\n",
      "1544-\n",
      "7601+\n",
      "3446+\n",
      "7733+\n",
      "2783+\n",
      "1374+\n",
      "504+\n",
      "1032+\n",
      "506-\n",
      "333-\n",
      "2473+\n",
      "904+\n",
      "6310+\n",
      "2832+\n",
      "4065-\n",
      "1393+\n",
      "3872-\n",
      "203-\n",
      "3633+\n",
      "277+\n",
      "<unk>\n",
      "484-\n",
      "98+\n",
      "3938-\n",
      "55-\n",
      "4285+\n",
      "3202+\n",
      "1444+\n",
      "2497+\n",
      "<unk>\n",
      "4640+\n",
      "<unk>\n",
      "3892+\n",
      "1260+\n",
      "4419+\n",
      "4540-\n",
      "6997+\n",
      "4798+\n",
      "6056+\n",
      "3981-\n",
      "1754-\n",
      "314+\n",
      "6260+\n",
      "4561+\n",
      "<unk>\n",
      "5324-\n",
      "7275-\n",
      "513+\n",
      "1258+\n",
      "<unk>\n",
      "2155+\n",
      "2308+\n",
      "3569+\n",
      "6360+\n",
      "2042-\n",
      "4153+\n"
     ]
    }
   ],
   "source": [
    "for x in converter(test_mot):\n",
    "    for i in x:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['320-'] par ['2224+']\n",
      "['<unk>'] par ['143-']\n",
      "['2610+'] par ['7833-']\n",
      "['910+'] par ['2037-']\n",
      "['2877+'] par ['83+']\n",
      "['443-'] par ['5122+']\n",
      "['470-'] par ['233-']\n",
      "['1985+'] par ['139+']\n",
      "['43+'] par ['1576+']\n",
      "['1398+'] par ['1429+']\n",
      "['1938+'] par ['1352+']\n",
      "['1644+'] par ['1644+']\n",
      "YEAH\n",
      "['1585+'] par ['2353+']\n",
      "['6405+'] par ['2257-']\n",
      "['994-'] par ['6434-']\n",
      "['2108-'] par ['7591+']\n",
      "['7493+'] par ['8358+']\n",
      "['3617+'] par ['8671+']\n",
      "['2048-'] par ['277+']\n",
      "['904+'] par ['1431+']\n",
      "['1437+'] par ['690+']\n",
      "['4402+'] par ['2832+']\n",
      "['7569+'] par ['6517+']\n",
      "['5686+'] par ['5653+']\n",
      "['897+'] par ['1306+']\n",
      "['1690+'] par ['931-']\n",
      "['3404-'] par ['1772+']\n",
      "['.'] par ['3590-']\n",
      "['900+'] par ['509-']\n",
      "['2588+'] par ['705+']\n",
      "['2803+'] par ['2579+']\n",
      "['6241+'] par ['2964+']\n",
      "['6520+'] par ['6729+']\n",
      "['948+'] par ['6764+']\n",
      "['276+'] par ['2373+']\n",
      "['4131+'] par ['702+']\n",
      "['1182-'] par ['4791+']\n",
      "['138-'] par ['1157-']\n",
      "['1059-'] par ['9+']\n",
      "['1971-'] par ['509-']\n",
      "['3814+'] par ['5150-']\n",
      "['3543-'] par ['4791+']\n",
      "['166-'] par ['910+']\n",
      "['3910+'] par ['156-']\n",
      "['6429-'] par ['1398-']\n",
      "['4145+'] par ['1754-']\n",
      "['6994+'] par ['3979+']\n",
      "['6901-'] par ['630-']\n",
      "['2512+'] par ['6835+']\n",
      "['5150+'] par ['2411+']\n",
      "['2700+'] par ['7505-']\n",
      "['123-'] par ['6501+']\n",
      "['199+'] par ['1881+']\n",
      "['2257+'] par ['2391+']\n",
      "['2625+'] par ['2144+']\n",
      "['989+'] par ['2485-']\n",
      "['3189+'] par ['6693+']\n",
      "['3845+'] par ['899-']\n",
      "['584-'] par ['2995+']\n",
      "['4007+'] par ['157-']\n",
      "['455-'] par ['6058+']\n",
      "['6405+'] par ['1057-']\n",
      "['314-'] par ['92+']\n",
      "['6405+'] par ['1938-']\n",
      "['1208-'] par ['92+']\n",
      "['986+'] par ['1474+']\n",
      "['302-'] par ['681-']\n",
      "['<unk>'] par ['138-']\n",
      "['<unk>'] par ['2030+']\n",
      "['4135+'] par ['2030+']\n",
      "['<unk>'] par ['2390+']\n",
      "['4450+'] par ['2030+']\n",
      "['<unk>'] par ['3037+']\n",
      "['<unk>'] par ['2030+']\n",
      "['1635+'] par ['2030+']\n",
      "['6423+'] par ['4661+']\n",
      "['6517+'] par ['3414-']\n",
      "['3002+'] par ['6513-']\n",
      "['1776+'] par ['4636+']\n",
      "['3460+'] par ['896-']\n",
      "['2592-'] par ['8447+']\n",
      "['3623+'] par ['1021-']\n",
      "['4789+'] par ['9367+']\n",
      "['1996-'] par ['1437+']\n",
      "['398+'] par ['2492+']\n",
      "['6544-'] par ['2144+']\n",
      "['3814+'] par ['3898-']\n",
      "['95-'] par ['1486-']\n",
      "['6225-'] par ['2770-']\n",
      "['6074-'] par ['4519+']\n",
      "['157+'] par ['6240-']\n",
      "['908-'] par ['197+']\n",
      "['6465+'] par ['133-']\n",
      "['1986+'] par ['6505+']\n",
      "['2315+'] par ['2036-']\n",
      "['4-'] par ['999+']\n",
      "['2514+'] par ['527-']\n",
      "['190+'] par ['2288+']\n",
      "['3635+'] par ['622+']\n",
      "['7355+'] par ['2030+']\n",
      "['186-'] par ['2019-']\n",
      "['751+'] par ['1477-']\n",
      "['9203+'] par ['508+']\n",
      "['508+'] par ['.']\n",
      "['3007+'] par ['337+']\n",
      "['483+'] par ['3641-']\n",
      "['436-'] par ['40-']\n",
      "['287+'] par ['184+']\n",
      "['4328+'] par ['706+']\n",
      "['1428+'] par ['4791+']\n",
      "['224+'] par ['1881+']\n",
      "['5895-'] par ['1627-']\n",
      "['3867+'] par ['5722-']\n",
      "['5938+'] par ['4935+']\n",
      "['1822+'] par ['706+']\n",
      "['1810-'] par ['1283+']\n",
      "['116-'] par ['1803-']\n",
      "['694+'] par ['2030+']\n",
      "['1035-'] par ['4791+']\n",
      "['887+'] par ['976+']\n",
      "['512+'] par ['3319+']\n",
      "['1916+'] par ['1283+']\n",
      "['7086+'] par ['2290+']\n",
      "['963+'] par ['5901-']\n",
      "['2654+'] par ['1290+']\n",
      "['1307+'] par ['2798+']\n",
      "['<unk>'] par ['615-']\n",
      "['509-'] par ['2030+']\n",
      "['1041+'] par ['508-']\n",
      "['2593+'] par ['8335+']\n",
      "['2390-'] par ['322+']\n",
      "['92+'] par ['1729+']\n",
      "['5869+'] par ['2006-']\n",
      "['1223-'] par ['5988+']\n",
      "['133+'] par ['1322+']\n",
      "['510+'] par ['329-']\n",
      "['3136+'] par ['418-']\n",
      "['4900+'] par ['2832+']\n",
      "['1057+'] par ['3557+']\n",
      "['2067-'] par ['2832+']\n",
      "['2391+'] par ['2285-']\n",
      "['3123+'] par ['2030+']\n",
      "['124-'] par ['2049+']\n",
      "['1544-'] par ['.']\n",
      "['7601+'] par ['789-']\n",
      "['3446+'] par ['2028-']\n",
      "['7733+'] par ['1031+']\n",
      "['2783+'] par ['472-']\n",
      "['1374+'] par ['957+']\n",
      "['504+'] par ['.']\n",
      "['1032+'] par ['2013-']\n",
      "['506-'] par ['2650+']\n",
      "['333-'] par ['307+']\n",
      "['2473+'] par ['461+']\n",
      "['904+'] par ['2262-']\n",
      "['6310+'] par ['31+']\n",
      "['2832+'] par ['4522+']\n",
      "['4065-'] par ['2761+']\n",
      "['1393+'] par ['2275-']\n",
      "['3872-'] par ['118-']\n",
      "['203-'] par ['2833-']\n",
      "['3633+'] par ['963-']\n",
      "['277+'] par ['4791+']\n",
      "['<unk>'] par ['659+']\n",
      "['484-'] par ['7752+']\n",
      "['98+'] par ['1777-']\n",
      "['3938-'] par ['35-']\n",
      "['55-'] par ['3935+']\n",
      "['4285+'] par ['1548+']\n",
      "['3202+'] par ['4646+']\n",
      "['1444+'] par ['472-']\n",
      "['2497+'] par ['2290+']\n",
      "['<unk>'] par ['825+']\n",
      "['4640+'] par ['7752+']\n",
      "['<unk>'] par ['4522+']\n",
      "['3892+'] par ['7752+']\n",
      "['1260+'] par ['963+']\n",
      "['4419+'] par ['3619-']\n",
      "['4540-'] par ['594-']\n",
      "['6997+'] par ['2449+']\n",
      "['4798+'] par ['5354+']\n",
      "['6056+'] par ['2391-']\n",
      "['3981-'] par ['6797-']\n",
      "['1754-'] par ['6576-']\n",
      "['314+'] par ['1231+']\n",
      "['6260+'] par ['418+']\n",
      "['4561+'] par ['152+']\n",
      "['<unk>'] par ['1627-']\n",
      "['5324-'] par ['2030+']\n",
      "['7275-'] par ['3312-']\n",
      "['513+'] par ['8004+']\n",
      "['1258+'] par ['355-']\n",
      "['<unk>'] par ['825+']\n",
      "['2155+'] par ['7752+']\n",
      "['2308+'] par ['2761+']\n",
      "['3569+'] par ['2214+']\n",
      "['6360+'] par ['4522+']\n",
      "['2042-'] par ['4522+']\n",
      "['4153+'] par ['831-']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,199):\n",
    "    prediction = best_model(test_mot).argmax(axis=2)\n",
    "    print(converter(test_mot)[i + 1], 'par', converter(prediction)[i])\n",
    "    if converter(test_mot)[i + 1] == converter(prediction)[i] :\n",
    "        print(\"YEAH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_mot = mot[0:200,:]\n",
    "test_mot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5421235253296321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#test_mot = mot\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "predictions = []\n",
    "true_values = []\n",
    "for i in range(0,199):\n",
    "    item = converter(test_mot[i+1])[0]\n",
    "    if item[len(item)-1] == '+' or item[len(item)-1] == '-':\n",
    "        plus_pred = best_model(test_mot)[i,:,unconverter(item[0:len(item)-1] + \"+\")]\n",
    "        minus_pred = best_model(test_mot)[i,:,unconverter(item[0:len(item)-1] + \"-\")]\n",
    "        plus_proba = sigmoid((plus_pred - minus_pred).detach())\n",
    "        predictions.append(plus_proba)\n",
    "        if item == item[0:len(item)-1] + \"+\" :\n",
    "            true_values.append(1)\n",
    "        else :\n",
    "            true_values.append(0)\n",
    "\n",
    "print(roc_auc_score(true_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.1451, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model(test_mot).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'314+'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = converter(test_mot[i+1])[0]\n",
    "item[0:len(item)-1] + \"+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconverter = np.vectorize(lambda x: TEXT.vocab.stoi[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['910+', '7399+', '217-', ..., '443+', '302+', '2832+'],\n",
       "       ['632+', '7911+', '2066-', ..., '2370+', '512-', '7355+'],\n",
       "       ['2125+', '6422-', '1034-', ..., '1544-', '197-', '4354+'],\n",
       "       ...,\n",
       "       ['4935+', '1622-', '921-', ..., '1730-', '3136+', '1251+'],\n",
       "       ['7450+', '1082-', '1497-', ..., '5753+', '6693+', '2761+'],\n",
       "       ['3673-', '1029-', '6191+', ..., '5705+', '4421+', '1429+']],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_out = best_model(train_data).argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converter(best_model(mot).argmax(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i, line in enumerate(converter(train_out.numpy())):\\n    print(''.join(line), pre_train[i], i)\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i, line in enumerate(converter(train_out.numpy())):\n",
    "    print(''.join(line), pre_train[i], i)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with the test dataset\n",
    "-------------------------------------\n",
    "\n",
    "Apply the best model to check the result with the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 14483]' is invalid for input of size 47029500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-ef381b7c3cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n\u001b[1;32m      4\u001b[0m     test_loss, math.exp(test_loss)))\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4d7151636315>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(eval_model, data_source)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 14483]' is invalid for input of size 47029500"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing M: (610 × 9724)\n",
      "Shapes (610, 20) (20, 9724)\n"
     ]
    }
   ],
   "source": [
    "'''import zero\n",
    "\n",
    "from zero.als import MangakiALS\n",
    "\n",
    "users_sorted = ratings_table.groupby(\"userId\").count().sort_values('rating')\n",
    "users_sorted_list = users_sorted.index.to_list()\n",
    "users_training_set = users_sorted_list[:math.floor(len(users_sorted_list)*0.9)]\n",
    "nb_users = len(users_sorted_list)\n",
    "movies_sorted = ratings_table.groupby(\"movieId\").count().sort_values('rating')\n",
    "movies_sorted_list = movies_sorted.index.to_list()\n",
    "nb_works = len(movies_sorted)\n",
    "users_test_set = users_sorted_list[math.floor(len(users_sorted_list)*0.9) :]\n",
    "training_ratings_table = ratings_table.query('userId in @users_training_set')\n",
    "X_train = np.array(training_ratings_table[['userId', 'movieId']])\n",
    "y_train = training_ratings_table['rating']\n",
    "als = MangakiALS(20)\n",
    "als.set_parameters(nb_users, nb_works)\n",
    "als.fit(X_train, y_train)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mean_of_user</th>\n",
       "      <th>scaled_rating</th>\n",
       "      <th>is_in_train</th>\n",
       "      <th>binary_ratings</th>\n",
       "      <th>mean_of_movie</th>\n",
       "      <th>mean_of_movie_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.920930</td>\n",
       "      <td>0.767442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.259615</td>\n",
       "      <td>0.442308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>-0.366379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.946078</td>\n",
       "      <td>0.745098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>0.633621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.975369</td>\n",
       "      <td>0.773399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>4.366379</td>\n",
       "      <td>0.633621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.237745</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>609</td>\n",
       "      <td>9416</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>0.311444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>609</td>\n",
       "      <td>9443</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>609</td>\n",
       "      <td>9444</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.633333</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>609</td>\n",
       "      <td>9445</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>1.311444</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>609</td>\n",
       "      <td>9485</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "      <td>3.688556</td>\n",
       "      <td>-0.688556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp  mean_of_user  scaled_rating  \\\n",
       "0            0        0     4.0   964982703      4.366379      -0.366379   \n",
       "1            0        2     4.0   964981247      4.366379      -0.366379   \n",
       "2            0        5     4.0   964982224      4.366379      -0.366379   \n",
       "3            0       43     5.0   964983815      4.366379       0.633621   \n",
       "4            0       46     5.0   964982931      4.366379       0.633621   \n",
       "...        ...      ...     ...         ...           ...            ...   \n",
       "100831     609     9416     4.0  1493848402      3.688556       0.311444   \n",
       "100832     609     9443     5.0  1493850091      3.688556       1.311444   \n",
       "100833     609     9444     5.0  1494273047      3.688556       1.311444   \n",
       "100834     609     9445     5.0  1493846352      3.688556       1.311444   \n",
       "100835     609     9485     3.0  1493846415      3.688556      -0.688556   \n",
       "\n",
       "        is_in_train  binary_ratings  mean_of_movie  mean_of_movie_binary  \n",
       "0                 1               1       3.920930              0.767442  \n",
       "1                 1               1       3.259615              0.442308  \n",
       "2                 1               1       3.946078              0.745098  \n",
       "3                 1               1       3.975369              0.773399  \n",
       "4                 1               1       4.237745              0.852941  \n",
       "...             ...             ...            ...                   ...  \n",
       "100831            1               1       3.333333              0.666667  \n",
       "100832            1               1       4.142857              0.857143  \n",
       "100833            1               1       3.633333              0.533333  \n",
       "100834            1               1       4.280000              0.920000  \n",
       "100835            1               0       2.333333              0.000000  \n",
       "\n",
       "[100836 rows x 10 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_table[\"binary_ratings\"] = ratings_table[\"rating\"].transform(lambda x : 1 if x>3 else 0)\n",
    "ratings_table[\"mean_of_movie\"] = ratings_table.groupby('movieId')[\"rating\"].transform('mean')\n",
    "ratings_table[\"mean_of_movie_binary\"] = ratings_table.groupby('movieId')[\"binary_ratings\"].transform('mean')\n",
    "ratings_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/pierre/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "ratings_table[\"is_in_train\"] = ratings_table[\"userId\"].transform(lambda x : 1 if x in users_dict else 0)\n",
    "training_table = ratings_table[ratings_table.is_in_train == 1]\n",
    "training_table[\"mean_of_movie\"] = training_table.groupby('movieId')[\"rating\"].transform('mean')\n",
    "training_table[\"mean_of_movie_binary\"] = training_table.groupby('movieId')[\"binary_ratings\"].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7609299097848716\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "true_values = []\n",
    "for i in range(0,199):\n",
    "    item = converter(test_mot[i+1])[0]\n",
    "    movieId = item[0:len(item)-1]\n",
    "    if item[len(item)-1] == '+' or item[len(item)-1] == '-':\n",
    "        plus_proba = list(training_table[training_table.movieId == int(movieId)]['mean_of_movie_binary'])[0]\n",
    "        predictions.append(plus_proba)\n",
    "        if item == movieId + \"+\" :\n",
    "            true_values.append(1)\n",
    "        else :\n",
    "            true_values.append(0)\n",
    "\n",
    "print(roc_auc_score(true_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
